{
  "aliases": [
    "scipy.stats.entropy"
  ],
  "arbitrary": [],
  "content": {
    "Attributes": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Extended Summary": {
      "children": [
        {
          "children": [
            {
              "type": "text",
              "value": "If only probabilities "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "pk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "pk"
            },
            {
              "type": "text",
              "value": " are given, the Shannon entropy is calculated as "
            },
            {
              "type": "inlineCode",
              "value": "H = -sum(pk * log(pk))"
            },
            {
              "type": "text",
              "value": "."
            }
          ],
          "type": "paragraph"
        },
        {
          "children": [
            {
              "type": "text",
              "value": "If "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "qk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "qk"
            },
            {
              "type": "text",
              "value": " is not None, then compute the relative entropy "
            },
            {
              "type": "inlineCode",
              "value": "D = sum(pk * log(pk / qk))"
            },
            {
              "type": "text",
              "value": ". This quantity is also known as the Kullback-Leibler divergence."
            }
          ],
          "type": "paragraph"
        },
        {
          "children": [
            {
              "type": "text",
              "value": "This routine will normalize "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "pk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "pk"
            },
            {
              "type": "text",
              "value": " and "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "qk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "qk"
            },
            {
              "type": "text",
              "value": " if they don't sum to 1."
            }
          ],
          "type": "paragraph"
        }
      ],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Methods": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Notes": {
      "children": [
        {
          "children": [
            {
              "type": "text",
              "value": "Informally, the Shannon entropy quantifies the expected uncertainty inherent in the possible outcomes of a discrete random variable. For example, if messages consisting of sequences of symbols from a set are to be encoded and transmitted over a noiseless channel, then the Shannon entropy "
            },
            {
              "type": "inlineCode",
              "value": "H(pk)"
            },
            {
              "type": "text",
              "value": " gives a tight lower bound for the average number of units of information needed per symbol if the symbols occur with frequencies governed by the discrete distribution "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "pk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "pk"
            },
            {
              "type": "text",
              "value": " . The choice of base determines the choice of units; e.g., "
            },
            {
              "type": "inlineCode",
              "value": "e"
            },
            {
              "type": "text",
              "value": " for nats, "
            },
            {
              "type": "inlineCode",
              "value": "2"
            },
            {
              "type": "text",
              "value": " for bits, etc."
            }
          ],
          "type": "paragraph"
        },
        {
          "children": [
            {
              "type": "text",
              "value": "The relative entropy, "
            },
            {
              "type": "inlineCode",
              "value": "D(pk|qk)"
            },
            {
              "type": "text",
              "value": ", quantifies the increase in the average number of units of information needed per symbol if the encoding is optimized for the probability distribution "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "qk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "qk"
            },
            {
              "type": "text",
              "value": " instead of the true distribution "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "pk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "pk"
            },
            {
              "type": "text",
              "value": ". Informally, the relative entropy quantifies the expected excess in surprise experienced if one believes the true distribution is "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "qk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "qk"
            },
            {
              "type": "text",
              "value": " when it is actually "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "pk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "pk"
            },
            {
              "type": "text",
              "value": "."
            }
          ],
          "type": "paragraph"
        },
        {
          "children": [
            {
              "type": "text",
              "value": "A related quantity, the cross entropy "
            },
            {
              "type": "inlineCode",
              "value": "CE(pk, qk)"
            },
            {
              "type": "text",
              "value": ", satisfies the equation "
            },
            {
              "type": "inlineCode",
              "value": "CE(pk, qk) = H(pk) + D(pk|qk)"
            },
            {
              "type": "text",
              "value": " and can also be calculated with the formula "
            },
            {
              "type": "inlineCode",
              "value": "CE = -sum(pk * log(qk))"
            },
            {
              "type": "text",
              "value": ". It gives the average number of units of information needed per symbol if an encoding is optimized for the probability distribution "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "qk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "qk"
            },
            {
              "type": "text",
              "value": " when the true distribution is "
            },
            {
              "anchor": null,
              "exists": true,
              "kind": "local",
              "reference": {
                "kind": "local",
                "module": null,
                "path": "pk",
                "type": "RefInfo",
                "version": null
              },
              "type": "Link",
              "value": "pk"
            },
            {
              "type": "text",
              "value": ". It is not computed directly by "
            },
            {
              "domain": null,
              "role": null,
              "type": "Directive",
              "value": "entropy"
            },
            {
              "type": "text",
              "value": ", but it can be computed using two calls to the function (see Examples)."
            }
          ],
          "type": "paragraph"
        },
        {
          "children": [
            {
              "type": "text",
              "value": "See  for more information."
            }
          ],
          "type": "paragraph"
        }
      ],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Other Parameters": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Parameters": {
      "children": [
        {
          "children": [
            {
              "desc": [
                {
                  "children": [
                    {
                      "type": "text",
                      "value": "Defines the (discrete) distribution. Along each axis-slice of "
                    },
                    {
                      "type": "inlineCode",
                      "value": "pk"
                    },
                    {
                      "type": "text",
                      "value": ", element "
                    },
                    {
                      "type": "inlineCode",
                      "value": "i"
                    },
                    {
                      "type": "text",
                      "value": " is the  (possibly unnormalized) probability of event "
                    },
                    {
                      "type": "inlineCode",
                      "value": "i"
                    },
                    {
                      "type": "text",
                      "value": "."
                    }
                  ],
                  "type": "paragraph"
                }
              ],
              "param": "pk",
              "type": "Param",
              "type_": "array_like"
            },
            {
              "desc": [
                {
                  "children": [
                    {
                      "type": "text",
                      "value": "Sequence against which the relative entropy is computed. Should be in the same format as "
                    },
                    {
                      "anchor": null,
                      "exists": true,
                      "kind": "local",
                      "reference": {
                        "kind": "local",
                        "module": null,
                        "path": "pk",
                        "type": "RefInfo",
                        "version": null
                      },
                      "type": "Link",
                      "value": "pk"
                    },
                    {
                      "type": "text",
                      "value": "."
                    }
                  ],
                  "type": "paragraph"
                }
              ],
              "param": "qk",
              "type": "Param",
              "type_": "array_like, optional"
            },
            {
              "desc": [
                {
                  "children": [
                    {
                      "type": "text",
                      "value": "The logarithmic base to use, defaults to "
                    },
                    {
                      "type": "inlineCode",
                      "value": "e"
                    },
                    {
                      "type": "text",
                      "value": " (natural logarithm)."
                    }
                  ],
                  "type": "paragraph"
                }
              ],
              "param": "base",
              "type": "Param",
              "type_": "float, optional"
            },
            {
              "desc": [
                {
                  "children": [
                    {
                      "type": "text",
                      "value": "The axis along which the entropy is calculated. Default is 0."
                    }
                  ],
                  "type": "paragraph"
                }
              ],
              "param": "axis",
              "type": "Param",
              "type_": "int, optional"
            }
          ],
          "type": "Parameters"
        }
      ],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Raises": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Receives": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Returns": {
      "children": [
        {
          "children": [
            {
              "desc": [
                {
                  "children": [
                    {
                      "type": "text",
                      "value": "The calculated entropy."
                    }
                  ],
                  "type": "paragraph"
                }
              ],
              "param": "S",
              "type": "Param",
              "type_": "{float, array_like}"
            }
          ],
          "type": "Parameters"
        }
      ],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Summary": {
      "children": [
        {
          "children": [
            {
              "type": "text",
              "value": "Calculate the Shannon entropy/relative entropy of given distribution(s)."
            }
          ],
          "type": "paragraph"
        }
      ],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Warnings": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Warns": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    },
    "Yields": {
      "children": [],
      "level": 0,
      "target": null,
      "title": null,
      "type": "Section"
    }
  },
  "example_section_data": {
    "children": [
      {
        "type": "text",
        "value": "The outcome of a fair coin is the most uncertain:"
      },
      {
        "type": "code",
        "value": "import numpy as np\nfrom scipy.stats import entropy\nbase = 2  # work in units of bits\npk = np.array([1/2, 1/2])  # fair coin\nH = entropy(pk, base=base)\nH"
      },
      {
        "type": "code",
        "value": "H == -np.sum(pk * np.log(pk)) / np.log(base)"
      },
      {
        "type": "text",
        "value": "The outcome of a biased coin is less uncertain:"
      },
      {
        "type": "code",
        "value": "qk = np.array([9/10, 1/10])  # biased coin\nentropy(qk, base=base)"
      },
      {
        "type": "text",
        "value": "The relative entropy between the fair coin and biased coin is calculated\nas:"
      },
      {
        "type": "code",
        "value": "D = entropy(pk, qk, base=base)\nD"
      },
      {
        "type": "code",
        "value": "D == np.sum(pk * np.log(pk/qk)) / np.log(base)"
      },
      {
        "type": "text",
        "value": "The cross entropy can be calculated as the sum of the entropy and\nrelative entropy`:"
      },
      {
        "type": "code",
        "value": "CE = entropy(pk, base=base) + entropy(pk, qk, base=base)\nCE"
      },
      {
        "type": "code",
        "value": "CE == -np.sum(pk * np.log(qk)) / np.log(base)"
      }
    ],
    "level": 0,
    "target": null,
    "title": null,
    "type": "Section"
  },
  "item_file": "/scipy/stats/_entropy.py",
  "item_line": 17,
  "item_type": "<class 'function'>",
  "ordered_sections": [
    "Summary",
    "Extended Summary",
    "Parameters",
    "Returns",
    "Notes",
    "References",
    "Examples"
  ],
  "references": null,
  "see_also": [],
  "signature": {
    "type": "Signature",
    "value": "(pk: 'np.typing.ArrayLike', qk: 'Optional[np.typing.ArrayLike]' = None, base: 'Optional[float]' = None, axis: 'int' = 0) -> 'Union[np.number, np.ndarray]'"
  },
  "type": "DocBlob"
}