{
  "_content": {
    "Summary": {
      "children": [
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "Dogleg algorithm with rectangular trust regions for least-squares minimization."
                }
              }
            ],
            "inner": []
          }
        }
      ],
      "title": null
    },
    "Extended Summary": {
      "children": [
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "The description of the algorithm can be found in [Voglis]_. The algorithm does trust-region iterations, but the shape of trust regions is rectangular as opposed to conventional elliptical. The intersection of a trust region and an initial feasible region is again some rectangle. Thus, on each iteration a bound-constrained quadratic optimization problem is solved."
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "A quadratic problem is solved by well-known dogleg approach, where the function is minimized along piecewise-linear \"dogleg\" path [NumOpt]_, Chapter 4. If Jacobian is not rank-deficient then the function is decreasing along this path, and optimization amounts to simply following along this path as long as a point stays within the bounds. A constrained Cauchy step (along the anti-gradient) is considered for safety in rank deficient cases, in this situations the convergence might be slow."
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "If during iterations some variable hit the initial bound and the component of anti-gradient points outside the feasible region, then a next dogleg step won't make any progress. At this state such variables satisfy first-order optimality conditions and they are excluded before computing a next dogleg step."
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "Gauss-Newton step can be computed exactly by "
                }
              },
              {
                "type": "Directive",
                "data": {
                  "value": [
                    "numpy.linalg.lstsq"
                  ],
                  "domain": null,
                  "role": null
                }
              },
              {
                "type": "Words",
                "data": {
                  "value": " (for dense Jacobian matrices) or by iterative procedure "
                }
              },
              {
                "type": "Directive",
                "data": {
                  "value": [
                    "scipy.sparse.linalg.lsmr"
                  ],
                  "domain": null,
                  "role": null
                }
              },
              {
                "type": "Words",
                "data": {
                  "value": " (for dense and sparse matrices, or Jacobian being LinearOperator). The second option allows to solve very large problems (up to couple of millions of residuals on a regular PC), provided the Jacobian matrix is sufficiently sparse. But note that dogbox is not very good for solving problems with large number of constraints, because of variables exclusion-inclusion on each iteration (a required number of function evaluations might be high or accuracy of a solution will be poor), thus its large-scale usage is probably limited to unconstrained problems."
                }
              }
            ],
            "inner": []
          }
        }
      ],
      "title": null
    },
    "Parameters": {
      "children": [],
      "title": null
    },
    "Returns": {
      "children": [],
      "title": null
    },
    "Yields": {
      "children": [],
      "title": null
    },
    "Receives": {
      "children": [],
      "title": null
    },
    "Raises": {
      "children": [],
      "title": null
    },
    "Warns": {
      "children": [],
      "title": null
    },
    "Other Parameters": {
      "children": [],
      "title": null
    },
    "Attributes": {
      "children": [],
      "title": null
    },
    "Methods": {
      "children": [],
      "title": null
    },
    "Notes": {
      "children": [],
      "title": null
    },
    "Warnings": {
      "children": [],
      "title": null
    }
  },
  "refs": [],
  "ordered_sections": [
    "Summary",
    "Extended Summary",
    "References"
  ],
  "item_file": "/Users/bussonniermatthias/miniconda3/lib/python3.8/site-packages/scipy/optimize/_lsq/dogbox.py",
  "item_line": 0,
  "item_type": "<class 'module'>",
  "aliases": [
    "scipy.optimize._lsq.dogbox"
  ],
  "example_section_data": {
    "children": [],
    "title": null
  },
  "see_also": [],
  "signature": null,
  "references": [
    ".. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region Dogleg",
    "            Approach for Unconstrained and Bound Constrained Nonlinear",
    "            Optimization\", WSEAS International Conference on Applied",
    "            Mathematics, Corfu, Greece, 2004.",
    ".. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization, 2nd edition\"."
  ],
  "arbitrary": [
    {
      "children": [
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "Dogleg algorithm with rectangular trust regions for least - squares minimization. "
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "The description of the algorithm can be found in . The algorithm does trust - region iterations, but the shape of trust regions is rectangular as opposed to conventional elliptical. The intersection of a trust region and an initial feasible region is again some rectangle. Thus, on each iteration a bound - constrained quadratic optimization problem is solved. "
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "A quadratic problem is solved by well - known dogleg approach, where the function is minimized along piecewise - linear \"dogleg\" path , Chapter 4. If Jacobian is not rank - deficient then the function is decreasing along this path, and optimization amounts to simply following along this path as long as a point stays within the bounds. A constrained Cauchy step (along the anti - gradient) is considered for safety in rank deficient cases, in this situations the convergence might be slow. "
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "If during iterations some variable hit the initial bound and the component of anti - gradient points outside the feasible region, then a next dogleg step won' t make any progress. At this state such variables satisfy first - order optimality conditions and they are excluded before computing a next dogleg step. "
                }
              }
            ],
            "inner": []
          }
        },
        {
          "type": "Paragraph",
          "data": {
            "inline": [
              {
                "type": "Words",
                "data": {
                  "value": "Gauss - Newton step can be computed exactly by "
                }
              },
              {
                "type": "Directive",
                "data": {
                  "value": [
                    "numpy.linalg.lstsq"
                  ],
                  "domain": null,
                  "role": null
                }
              },
              {
                "type": "Words",
                "data": {
                  "value": " (for dense Jacobian matrices) or by iterative procedure "
                }
              },
              {
                "type": "Directive",
                "data": {
                  "value": [
                    "scipy.sparse.linalg.lsmr"
                  ],
                  "domain": null,
                  "role": null
                }
              },
              {
                "type": "Words",
                "data": {
                  "value": " (for dense and sparse matrices, or Jacobian being LinearOperator). The second option allows to solve very large problems (up to couple of millions of residuals on a regular PC), provided the Jacobian matrix is sufficiently sparse. But note that dogbox is not very good for solving problems with large number of constraints, because of variables exclusion - inclusion on each iteration (a required number of function evaluations might be high or accuracy of a solution will be poor), thus its large - scale usage is probably limited to unconstrained problems. "
                }
              }
            ],
            "inner": []
          }
        }
      ],
      "title": null
    },
    {
      "children": [],
      "title": "References"
    }
  ]
}